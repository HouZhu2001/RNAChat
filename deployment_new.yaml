apiVersion: v1
kind: Pod
metadata:
  name: rnachat-gpus # you prefered pod name
spec:
  volumes:
    - name: rnachat-data
      persistentVolumeClaim:
        claimName: rnachat-pvc
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 128Gi
  containers:
    - name: vol-container
      image: pytorch/pytorch:2.2.0-cuda11.8-cudnn8-devel # I recommend this image or continuumio/miniconda3
      workingDir: /
      command: ["/bin/bash", "-c"]
      args:
        - |
          export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True && \
          apt-get update && apt-get install -y git curl python3-pip && \
          pip install gdown && \
          git clone https://github.com/HouZhu2001/RNAChat.git && \
          gdown "https://drive.google.com/uc?export=download&id=1q5r6tFty2vQuDny4NVQqjVMkw5YyXRMn" -O /data/checkpoint_stage1.pth && \
          gdown "https://drive.google.com/uc?export=download&id=1jVHMCbbL236BK7khQ906JSuBNyPwV05o" -O /data/checkpoint_stage2.pth && \
          echo "Repo cloned and checkpoint downloaded!" && \
          cp -r /data/* /RNAChat/rnachat/checkpoints && \
          pip install -r /RNAChat/requirements.txt && \
          git clone https://github.com/lbcb-sci/RiNALMo.git /RNAChat/rnachat/rinalmo && \
          cd /RNAChat/rnachat/rinalmo && \
          pip install . && \
          MAX_JOBS=6 pip -v install flash-attn==2.3.2 --no-build-isolation && \
          pip install flash-attn==2.3.2 && \
          cd ../../ && \
          python3 train.py && \
          cp -r /RNAChat/rnachat/checkpoints/* /data/ && \
          tail -f /dev/null
      resources:
        limits:
          cpu: 16
          nvidia.com/gpu: 1
          memory: 32Gi
        requests:
          cpu: 16
          nvidia.com/gpu: 1
          memory: 32Gi
      volumeMounts:
        - mountPath: /data
          name: rnachat-data
        - mountPath: /dev/shm # mounted on /dev/shm
          name: dshm
  restartPolicy: Never
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - NVIDIA-A100-SXM4-80GB
                  - NVIDIA-H100-SXM
                  - NVIDIA-H100-PCIe
                  - NVIDIA-A100-PCIe-80GB
                  - NVIDIA-A100-80GB-PCIe-MIG-1g.10gb
                  - NVIDIA-A6000 # 48GB
                  - NVIDIA-RTX-A6000 # 48GB
                  # - NVIDIA-V100-SXM2-32GB
                  # - NVIDIA-V100-PCIe-32GB
                  - NVIDIA-A100-PCIE-40GB
                  # - Tesla-V100-SXM2-32GB
                  - Quadro-RTX-8000
                  - NVIDIA-GH200-480GB
                  - NVIDIA-L40
                  - NVIDIA-A40

                  # - NVIDIA-A100
                  # - NVIDIA-A40
                  # - NVIDIA-H100
                  # - NVIDIA-A6000
                  # - NVIDIA-A100
                  # - NVIDIA-A100-SXM4-40GB
                  # - NVIDIA-A100-SXM4-80GB
                  # - NVIDIA-H100-PCIe
                  # - NVIDIA-H100-SXM
                  # - NVIDIA-RTX-A6000
                  # - NVIDIA-RTX-A5000
                  # - NVIDIA-RTX-6000-Ada
                  # - NVIDIA-3090
                  # - NVIDIA-4090
                  # - NVIDIA-V100
                  # - NVIDIA-V100-SXM2-32GB
                  # - NVIDIA-V100-PCIe-32GB
                  # - NVIDIA-A100-SXM4-80GB
                  # - NVIDIA-A100-PCIe-80GB
